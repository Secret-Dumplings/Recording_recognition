# -*- coding: utf-8 -*-
import importlib.util
import subprocess
import sys
import os
# å®šä¹‰éœ€è¦çš„åº“
required_packages = ['pyaudio', 'wave', 'pydub', 'funasr', 'numpy', 'emoji', "torch", "torchaudio", "PyQt5"]

# æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±çš„åº“
for package in required_packages:
    if importlib.util.find_spec(package) is None:
        print(f"{package} æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--index-url", "https://mirrors.aliyun.com/pypi/simple/"])
    else:
        # print(f"{package} å·²å®‰è£…")
        pass
print("æ‰€æœ‰ä¾èµ–åº“å‡å·²å®‰è£…")
# å¯¼å…¥éœ€è¦çš„åº“
import threading
from datetime import datetime
import time
import pyaudio
import wave
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import numpy as np
import emoji
from PyQt5.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QTextEdit,
    QMessageBox
)
from PyQt5.QtCore import QTimer

# è®¾ç½®ffmpegè·¯å¾„ï¼ˆå¦‚æœéœ€è¦ï¼‰
ffmpeg_path = r"C:\ffmpeg-master-latest-win64-gpl-shared\bin\ffmpeg.exe"
os.environ["PATH"] += os.pathsep + os.path.dirname(ffmpeg_path)

# é…ç½®æ¨¡å‹
model_dir = r"C:\Users\admin\.cache\modelscope\hub\iic\SenseVoiceSmall"
vad_model_dir = r"C:\Users\admin\.cache\modelscope\hub\iic\speech_fsmn_vad_zh-cn-16k-common-pytorch"
model = AutoModel(
    model=model_dir,
    vad_model=vad_model_dir,
    trust_remote_code=False,
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
    disable_update=True
)

# éŸ³é¢‘å½•åˆ¶å‚æ•°
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024
RECORD_SECONDS = 5  # æ¯æ¬¡è¯†åˆ«çš„éŸ³é¢‘ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰

# å®šä¹‰è¡¨æƒ…ç¬¦å·æ˜ å°„è¡¨
emoji_map = {
    "ğŸ˜Š": "(é«˜å…´)",
    "ğŸ˜¡": "(æ„¤æ€’/å…´å¥‹)",
    "ğŸ˜”": "(æ‚²ä¼¤)",
    "ğŸ˜€": "(ç¬‘å£°)",
    "ğŸ¼": "(éŸ³ä¹)",
    "ğŸ‘": "(æŒå£°)",
    "ğŸ¤§": "(å’³å—½å’Œæ‰“å–·åš)",
    "ğŸ˜­": "(å“­æ³£)"
}

all_audio_dir = []

class AudioRecorder:
    def __init__(self, input_device_index):
        self.input_device_index = input_device_index
        self.audio = pyaudio.PyAudio()
        self.stream = None
        self.recording = False
        self.frames = []

    def start_recording(self):
        self.recording = True
        self.frames = []
        self.stream = self.audio.open(
            format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK,
            input_device_index=self.input_device_index
        )
        print("å¼€å§‹å½•åˆ¶...")

    def stop_recording(self):
        self.recording = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.audio.terminate()
            print("å½•åˆ¶ç»“æŸ...")

    def record_chunk(self):
        if self.recording:
            data = self.stream.read(CHUNK)
            self.frames.append(data)
            return data
        return None

    def save_audio(self, audio_dir):
        now = datetime.now()
        timestamp = now.strftime("%Y%m%d_%H%M%S")
        audio_file_name = f"recorded_audio_{timestamp}.wav"
        wave_file = os.path.join(audio_dir, audio_file_name)

        wf = wave.open(wave_file, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(self.audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(self.frames))
        wf.close()
        all_audio_dir.append(str(wave_file))
        return wave_file

def recognize_audio(audio_data):
    try:
        # å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_audio_file = "temp_audio.wav"
        wf = wave.open(temp_audio_file, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(2)  # 16-bit
        wf.setframerate(RATE)
        wf.writeframes(audio_data)
        wf.close()

        res = model.generate(
            input=temp_audio_file,
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=True,
            merge_length_s=15,
        )
        if res and isinstance(res, list) and len(res) > 0:
            full_text = ' '.join([segment["text"] for segment in res])
            full_text = rich_transcription_postprocess(full_text)

            # æ›¿æ¢æŒ‡å®šçš„è¡¨æƒ…ç¬¦å·
            for emoji_char, description in emoji_map.items():
                full_text = full_text.replace(emoji_char, description)

            # å»é™¤å…¶ä»–è¡¨æƒ…ç¬¦å·
            full_text = ''.join(char for char in full_text if not emoji.is_emoji(char))

            return full_text
        else:
            return ""
    except Exception as e:
        print(f"éŸ³é¢‘è¯†åˆ«é”™è¯¯: {e}")
        return ""

class AudioApp(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.input_device_index = 1  # é€‰æ‹©æ­£ç¡®çš„è®¾å¤‡ç´¢å¼•
        self.audio_dir = None
        self.recorder = AudioRecorder(self.input_device_index)
        self.recording_thread = None
        self.recording = False
        self.text_area.append("secret_dumplingså‡ºå“\næ¬¢è¿ä½¿ç”¨\né¸£è°¢ï¼šmodelscopeç¤¾åŒº@é€šä¹‰å®éªŒå®¤æä¾›çš„SenseVoiceæ¨¡å‹")

    def initUI(self):
        self.setWindowTitle("éŸ³é¢‘å½•åˆ¶ä¸å®æ—¶è¯†åˆ«")
        self.setGeometry(100, 100, 600, 400)

        layout = QVBoxLayout()

        self.label = QLabel("éŸ³é¢‘å½•åˆ¶ä¸å®æ—¶è¯†åˆ«å·¥å…·")
        layout.addWidget(self.label)

        self.text_area = QTextEdit()
        self.text_area.setReadOnly(True)
        layout.addWidget(self.text_area)

        self.record_button = QPushButton("å¼€å§‹å½•åˆ¶")
        self.record_button.clicked.connect(self.toggle_recording)
        layout.addWidget(self.record_button)
        self.audio_dir = None  # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œå°† audio_dir è®¾ç½®ä¸º None
        self.setLayout(layout)

    def toggle_recording(self):
        if not self.recording:
            self.start_recording()
        else:
            self.stop_recording()
            # åˆå¹¶éŸ³é¢‘æ–‡ä»¶
            self.concatenate_audios(os.listdir(self.audio_dir), os.path.join(self.audio_dir, "recorded_audio.wav"))

    def start_recording(self):
        if self.audio_dir is None:
            # è‡ªåŠ¨åˆ›å»ºç›®å½•
            now = datetime.now()
            self.audio_dir = now.strftime("./output/AUDIO_DIR_%Y%m%d_%H%M%S")
            try:
                os.makedirs(r"./output", exist_ok=True)
                os.makedirs(self.audio_dir, exist_ok=True)
                self.text_area.append(f"éŸ³é¢‘ä¿å­˜ç›®å½•å·²åˆ›å»º: {self.audio_dir}")
            except Exception as e:
                self.text_area.append(f"åˆ›å»ºéŸ³é¢‘ä¿å­˜ç›®å½•å¤±è´¥: {e}")

        self.recording = True
        self.record_button.setText("åœæ­¢å½•åˆ¶")
        self.text_area.append("æ­£åœ¨å½•åˆ¶éŸ³é¢‘...")
        self.recording_thread = threading.Thread(target=self.record_audio_thread)
        self.recording_thread.start()

    def stop_recording(self):
        self.recording = False
        self.record_button.setText("å¼€å§‹å½•åˆ¶")
        self.text_area.append("å½•åˆ¶ç»“æŸ")
        self.recorder.stop_recording()

    def record_audio_thread(self):
        print(f"éŸ³é¢‘ä¿å­˜ç›®å½•: {self.audio_dir}")
        if self.audio_dir is None:
            print("éŸ³é¢‘ä¿å­˜ç›®å½•æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ç¨‹åºé€»è¾‘ï¼")
            return

        self.recorder.start_recording()
        while self.recording:
            for _ in range(int(RATE / CHUNK * RECORD_SECONDS)):
                self.recorder.record_chunk()
            audio_data = b''.join(self.recorder.frames)
            text = recognize_audio(audio_data)
            if text:
                self.text_area.append(f"å®æ—¶è¯†åˆ«ç»“æœ: {text}")
                # ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶
                self.recorder.save_audio(self.audio_dir)
                # ä¿å­˜è¯†åˆ«ç»“æœåˆ°æ–‡ä»¶
                text_file_path = os.path.join(all_audio_dir, "recognized_text.txt")
                with open(text_file_path, "a", encoding="utf-8-sig", errors="replace") as f:
                    f.write(f"{text}\n")
            self.recorder.frames = []  # æ¸…ç©ºå·²å¤„ç†çš„éŸ³é¢‘å¸§
            time.sleep(0.1)  # æ§åˆ¶è¯†åˆ«é¢‘ç‡

    def concatenate_audios(self, file_list, output_file):
        """åˆå¹¶å¤šä¸ªéŸ³é¢‘æ–‡ä»¶åˆ°ä¸€ä¸ªæ–‡ä»¶"""
        from pydub import AudioSegment
        combined = AudioSegment.empty()
        for file in file_list:
            try:
                audio = AudioSegment.from_wav(file)
                combined += audio
            except Exception as e:
                pass
                # print(f"æ— æ³•å¤„ç†æ–‡ä»¶ {file}: {e}")
        try:
            combined.export(output_file, format="wav")
            print("åˆå¹¶å®Œæˆã€‚")
        except Exception as e:
            print(f"å¯¼å‡ºé”™è¯¯: {e}")

if __name__ == "__main__":
    try:
        app = QApplication(sys.argv)
        window = AudioApp()
        window.show()
        sys.exit(app.exec_())
    except Exception as e:
        print(f"ç¨‹åºå¯åŠ¨å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {e}")
