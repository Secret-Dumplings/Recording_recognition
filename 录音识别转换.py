# -*- coding: utf-8 -*-
import sys
import subprocess
import importlib.util

# å®šä¹‰éœ€è¦çš„åº“
required_packages = ['pyaudio', 'wave', 'pydub', 'funasr', 'numpy', 'emoji', "torch", "torchaudio"]

# æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±çš„åº“
for package in required_packages:
    if importlib.util.find_spec(package) is None:
        print(f"{package} æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--index-url", "https://mirrors.aliyun.com/pypi/simple/"])
    else:
        print(f"{package} å·²å®‰è£…")

# å¯¼å…¥æ‰€éœ€åº“
import pyaudio
import wave
import pydub
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import os
from datetime import datetime
import numpy as np
import emoji

# è®¾ç½®ffmpegè·¯å¾„
ffmpeg_path = r"C:\ffmpeg-master-latest-win64-gpl-shared\bin\ffmpeg.exe"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
os.environ["PATH"] += os.pathsep + os.path.dirname(ffmpeg_path)
pydub.AudioSegment.ffmpeg = ffmpeg_path

# é…ç½®æ¨¡å‹
model_dir = "iic/SenseVoiceSmall"
model = AutoModel(
    model=model_dir,
    trust_remote_code=True,
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# éŸ³é¢‘å½•åˆ¶å‚æ•°
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024
RECORD_SECONDS = 10  # æ¯æ¬¡å½•åˆ¶çš„æ—¶é—´ï¼ˆç§’ï¼‰

# å®šä¹‰è¡¨æƒ…ç¬¦å·æ˜ å°„è¡¨
emoji_map = {
    "ğŸ˜Š": "(é«˜å…´)",
    "ğŸ˜¡": "(æ„¤æ€’/å…´å¥‹)",
    "ğŸ˜”": "(æ‚²ä¼¤)",
    "ğŸ˜€": "(ç¬‘å£°)",
    "ğŸ¼": "(éŸ³ä¹)",
    "ğŸ‘": "(æŒå£°)",
    "ğŸ¤§": "(å’³å—½å’Œæ‰“å–·åš)",
    "ğŸ˜­": "(å“­æ³£)"
}

def record_audio(input_device_index, audio_dir):
    """å½•åˆ¶éŸ³é¢‘å¹¶ä¿å­˜åˆ°å”¯ä¸€æ–‡ä»¶å"""
    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK,
                        input_device_index=input_device_index)
    print("å¼€å§‹å½•åˆ¶...")
    frames = []

    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)

    print("å½•åˆ¶ç»“æŸ...")
    stream.stop_stream()
    stream.close()
    audio.terminate()

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    now = datetime.now()
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    audio_file_name = f"recorded_audio_{timestamp}.wav"
    wave_file = os.path.join(audio_dir, audio_file_name)

    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    wf = wave.open(wave_file, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(audio.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

    # åº”ç”¨å¢ç›Šæé«˜éŸ³é‡
    audio_segment = pydub.AudioSegment.from_wav(wave_file)
    audio_segment = audio_segment.apply_gain(30)  # å¢åŠ 30åˆ†è´
    audio_segment.export(wave_file, format="wav")

    # é‡æ–°è¯»å–å¢ç›Šå¤„ç†åçš„éŸ³é¢‘æ•°æ®
    audio_segment_processed = pydub.AudioSegment.from_wav(wave_file)
    audio_data_processed = audio_segment_processed.raw_data
    audio_array_processed = np.frombuffer(audio_data_processed, dtype=np.int16)

    # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
    if np.isnan(audio_array_processed).any():
        print("éŸ³é¢‘æ•°æ®åŒ…å«NaNå€¼ï¼Œè¿›è¡Œå¤„ç†...")
        audio_array_processed = np.nan_to_num(audio_array_processed)

    # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦å¤ªå®‰é™
    if np.max(np.abs(audio_array_processed)) < 10:
        print("Recording is too quiet. Skipping.")
        return None

    rms = np.sqrt(np.mean(audio_array_processed ** 2))
    print(f"RMS value after gain: {rms}")
    return wave_file

def recognize_audio(audio_file):
    try:
        res = model.generate(
            input=audio_file,
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=True,
            merge_length_s=15,
        )
        if res and isinstance(res, list) and len(res) > 0:
            full_text = ' '.join([segment["text"] for segment in res])
            full_text = rich_transcription_postprocess(full_text)

            # æ›¿æ¢æŒ‡å®šçš„è¡¨æƒ…ç¬¦å·
            for emoji_char, description in emoji_map.items():
                full_text = full_text.replace(emoji_char, description)

            # å»é™¤å…¶ä»–è¡¨æƒ…ç¬¦å·
            full_text = ''.join(char for char in full_text if not emoji.is_emoji(char))

            return full_text
        else:
            return ""
    except Exception as e:
        print(f"éŸ³é¢‘è¯†åˆ«é”™è¯¯: {e}")
        return ""

def concatenate_audios(file_list, output_file):
    """åˆå¹¶å¤šä¸ªéŸ³é¢‘æ–‡ä»¶åˆ°ä¸€ä¸ªæ–‡ä»¶"""
    from pydub import AudioSegment
    combined = AudioSegment.empty()
    for file in file_list:
        try:
            audio = AudioSegment.from_wav(file)
            combined += audio
        except Exception as e:
            print(f"æ— æ³•å¤„ç†æ–‡ä»¶ {file}: {e}")
    try:
        combined.export(output_file, format="wav")
        print("åˆå¹¶å®Œæˆã€‚")
    except Exception as e:
        print(f"å¯¼å‡ºé”™è¯¯: {e}")

def main():
    recorded_files = []
    input_device_index = 1  # é€‰æ‹©æ­£ç¡®çš„è®¾å¤‡ç´¢å¼•

    # ç”ŸæˆåŸºäºæ—¶é—´æˆ³çš„ç›®å½•å
    now = datetime.now()
    timestamp_dir = now.strftime("AUDIO_DIR_%Y%m%d_%H%M%S")
    os.makedirs(timestamp_dir, exist_ok=True)

    try:
        while True:
            print("å‡†å¤‡å½•åˆ¶éŸ³é¢‘...")
            audio_file = record_audio(input_device_index, timestamp_dir)
            if audio_file:
                print(f"å½•åˆ¶æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {audio_file}")
                recorded_files.append(audio_file)
                print("å¼€å§‹è¯†åˆ«éŸ³é¢‘...")
                text = recognize_audio(audio_file)
                print(f"è¯†åˆ«ç»“æœ: {text}")

                # ä¿å­˜è¯†åˆ«ç»“æœåˆ°æ–‡ä»¶ï¼ŒæŒ‡å®šencoding="utf-8-sig"
                text_file_path = os.path.join(timestamp_dir, "recognized_text.txt")
                with open(text_file_path, "a", encoding="utf-8-sig", errors="replace") as f:
                    f.write(f"{text}\n")
            else:
                print("å½•éŸ³å¤ªå®‰é™ï¼Œè·³è¿‡...")
    except KeyboardInterrupt:
        print("ç¨‹åºå·²æ‰‹åŠ¨ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

    # å½•åˆ¶ç»“æŸåï¼Œåˆå¹¶æ‰€æœ‰å½•éŸ³æ–‡ä»¶
    if recorded_files:
        print(f"Recorded files: {recorded_files}")
        output_file = os.path.join(timestamp_dir, "final_audio.wav")
        concatenate_audios(recorded_files, output_file)
        print(f"æ‰€æœ‰å½•éŸ³å·²åˆå¹¶åˆ° {output_file}")

if __name__ == "__main__":
    main()